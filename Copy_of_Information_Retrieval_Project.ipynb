{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm4pDLyfprSHc60f4fiBIq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hassanashraf20/E-SHOP/blob/main/Copy_of_Information_Retrieval_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZwFAvui4A6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7e6b9d0-3850-486e-b7a2-f030f14cff86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# import the necessary libraries\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk import pos_tag\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Make sure to download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "\n",
        "documents = [\n",
        "    \"She sells seashells by the seashore.\",\n",
        "    \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\",\n",
        "    \"To be or not to be, that is the question.\",\n",
        "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",\n",
        "    \"Roses are red, violets are blue, sugar is sweet, and so are you.\",\n",
        "    \"The cat in the hat sat on the mat.\",\n",
        "    \"All that glitters is not gold.\",\n",
        "    \"It was the best of times, it was the worst of times.\",\n",
        "    \"In a hole in the ground, there lived a hobbit.\",\n",
        "    \"To be or not to be, that is the question.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"She sells seashells by the seashore.\",\n",
        "    \"How much wood would a woodchuck ?\",\n",
        "    \"Hey, did you know that the summer break is coming? Amazing right!! It's only 5 more days!!\",\n",
        "    \"There are 3 balls in this bag, and 12 in the other one.\",\n",
        "    \"To be or not to be, that is the question.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"She sells seashells by the seashore.\",\n",
        "    \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\",\n",
        "    \"To be or not to be, that is the question.\",\n",
        "    \"we don't need   the given questions\",\n",
        "    \"This is a sample sentence and we are going to remove the stopwords from this.\",\n",
        "    \"data science uses scientific methods algorithms and many types of processes\",\n",
        "    \"n this article, we will learn by using various Python Libraries and Techniques that are involved in Text Processing.\",\n",
        "    \"There are 66 bell in this bag, and 33 in the other one.\",\n",
        "    \" Gettysburg Address by Abraham Lincoln\",\n",
        "    \" Declaration of Independence by Thomas Jefferson\",\n",
        "    \" Emancipation Proclamation by Abraham Lincoln\",\n",
        "    \"The Magna Carta \",\n",
        "    \"The Bill of Rights \"\n",
        "    \"The Emancipation Proclamation \",\n",
        "    \"The Gettysburg Address \",\n",
        "    \" Emancipation Proclamation \",\n",
        "    \"The 19th Amendment \",\n",
        "    \"The Social Security Act \",\n",
        "    \"The Civil Rights Act \",\n",
        "    \"The Voting Rights Act \",\n",
        "    \"The Fair Housing Act \",\n",
        "    \" Americans with Disabilities Act \",\n",
        "    \"The Affordable Care Act \",\n",
        "    \" Dodd-Frank Wall Street Reform and Consumer Protection Act \",\n",
        "    \"The Tax Cuts and Jobs Act\"\n",
        "]\n",
        "\n",
        "query = [\"She sells seashells by the seashore\"]\n",
        "\n",
        "# Preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to convert NLTK's part of speech tags to wordnet tags\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun if unknown\n",
        "\n",
        "def preprocess(document):\n",
        "    words = nltk.word_tokenize(document.lower())\n",
        "    pos_tags = pos_tag(words)\n",
        "    filtered_words = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in pos_tags\n",
        "        if word not in stop_words and word.isalnum()\n",
        "    ]\n",
        "    return filtered_words\n",
        "\n",
        "preprocessed_docs = [preprocess(doc) for doc in docs]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Preprocessing\n",
        "# Text preprocessing functions\n",
        "def text_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_numbers(text):\n",
        "    result = re.sub(r'\\d+', '', text)\n",
        "    return result\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Apply preprocessing to each document\n",
        "preprocessed_documents = []\n",
        "for doc in documents:\n",
        "    doc = text_lowercase(doc)\n",
        "    doc = remove_numbers(doc)\n",
        "    doc = remove_punctuation(doc)\n",
        "    preprocessed_documents.append(doc)\n",
        "\n",
        "print(preprocessed_documents)\n"
      ],
      "metadata": {
        "id": "GQHoJNBVLjCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f0b8db4-dff4-417f-abcb-f8f677145110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['she sells seashells by the seashore', 'how much wood would a woodchuck chuck if a woodchuck could chuck wood', 'to be or not to be that is the question', 'lorem ipsum dolor sit amet consectetur adipiscing elit', 'roses are red violets are blue sugar is sweet and so are you', 'the cat in the hat sat on the mat', 'all that glitters is not gold', 'it was the best of times it was the worst of times', 'in a hole in the ground there lived a hobbit', 'to be or not to be that is the question', 'the quick brown fox jumps over the lazy dog', 'she sells seashells by the seashore', 'how much wood would a woodchuck ', 'hey did you know that the summer break is coming amazing right its only  more days', 'there are  balls in this bag and  in the other one', 'to be or not to be that is the question', 'the quick brown fox jumps over the lazy dog', 'she sells seashells by the seashore', 'how much wood would a woodchuck chuck if a woodchuck could chuck wood', 'to be or not to be that is the question', 'we dont need   the given questions', 'this is a sample sentence and we are going to remove the stopwords from this', 'data science uses scientific methods algorithms and many types of processes', 'n this article we will learn by using various python libraries and techniques that are involved in text processing', 'there are  bell in this bag and  in the other one', ' gettysburg address by abraham lincoln', ' declaration of independence by thomas jefferson', ' emancipation proclamation by abraham lincoln', 'the magna carta ', 'the bill of rights the emancipation proclamation ', 'the gettysburg address ', ' emancipation proclamation ', 'the th amendment ', 'the social security act ', 'the civil rights act ', 'the voting rights act ', 'the fair housing act ', ' americans with disabilities act ', 'the affordable care act ', ' doddfrank wall street reform and consumer protection act ', 'the tax cuts and jobs act']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in preprocessed_documents]\n",
        "print(lemmatized_tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "rhZOfKpRA_Fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bfd7e7e-fdbb-4a11-b768-3d58f065d104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['she sells seashells by the seashore', 'how much wood would a woodchuck chuck if a woodchuck could chuck wood', 'to be or not to be that is the question', 'lorem ipsum dolor sit amet consectetur adipiscing elit', 'roses are red violets are blue sugar is sweet and so are you', 'the cat in the hat sat on the mat', 'all that glitters is not gold', 'it was the best of times it was the worst of times', 'in a hole in the ground there lived a hobbit', 'to be or not to be that is the question', 'the quick brown fox jumps over the lazy dog', 'she sells seashells by the seashore', 'how much wood would a woodchuck ', 'hey did you know that the summer break is coming amazing right its only  more days', 'there are  balls in this bag and  in the other one', 'to be or not to be that is the question', 'the quick brown fox jumps over the lazy dog', 'she sells seashells by the seashore', 'how much wood would a woodchuck chuck if a woodchuck could chuck wood', 'to be or not to be that is the question', 'we dont need   the given questions', 'this is a sample sentence and we are going to remove the stopwords from this', 'data science uses scientific methods algorithms and many types of processes', 'n this article we will learn by using various python libraries and techniques that are involved in text processing', 'there are  bell in this bag and  in the other one', ' gettysburg address by abraham lincoln', ' declaration of independence by thomas jefferson', ' emancipation proclamation by abraham lincoln', 'the magna carta ', 'the bill of rights the emancipation proclamation ', 'the gettysburg address ', ' emancipation proclamation ', 'the th amendment ', 'the social security act ', 'the civil rights act ', 'the voting rights act ', 'the fair housing act ', ' americans with disabilities act ', 'the affordable care act ', ' doddfrank wall street reform and consumer protection act ', 'the tax cuts and jobs act']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the documents\n",
        "term_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
        "\n",
        "# Get the terms (features)\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the term matrix\n",
        "print(term_matrix.todense())\n",
        "print(\"\\n\")\n",
        "print(\"Terms:\\n\", terms)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LgF-UPcgR8-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb5c2b1-38be-4456-b17b-bb642d3863f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]]\n",
            "\n",
            "\n",
            "Terms:\n",
            " ['abraham' 'act' 'address' 'adipiscing' 'affordable' 'algorithms' 'all'\n",
            " 'amazing' 'amendment' 'americans' 'amet' 'and' 'are' 'article' 'bag'\n",
            " 'balls' 'be' 'bell' 'best' 'bill' 'blue' 'break' 'brown' 'by' 'care'\n",
            " 'carta' 'cat' 'chuck' 'civil' 'coming' 'consectetur' 'consumer' 'could'\n",
            " 'cuts' 'data' 'days' 'declaration' 'did' 'disabilities' 'doddfrank' 'dog'\n",
            " 'dolor' 'dont' 'elit' 'emancipation' 'fair' 'fox' 'from' 'gettysburg'\n",
            " 'given' 'glitters' 'going' 'gold' 'ground' 'hat' 'hey' 'hobbit' 'hole'\n",
            " 'housing' 'how' 'if' 'in' 'independence' 'involved' 'ipsum' 'is' 'it'\n",
            " 'its' 'jefferson' 'jobs' 'jumps' 'know' 'lazy' 'learn' 'libraries'\n",
            " 'lincoln' 'lived' 'lorem' 'magna' 'many' 'mat' 'methods' 'more' 'much'\n",
            " 'need' 'not' 'of' 'on' 'one' 'only' 'or' 'other' 'over' 'processes'\n",
            " 'processing' 'proclamation' 'protection' 'python' 'question' 'questions'\n",
            " 'quick' 'red' 'reform' 'remove' 'right' 'rights' 'roses' 'sample' 'sat'\n",
            " 'science' 'scientific' 'seashells' 'seashore' 'security' 'sells'\n",
            " 'sentence' 'she' 'sit' 'so' 'social' 'stopwords' 'street' 'sugar'\n",
            " 'summer' 'sweet' 'tax' 'techniques' 'text' 'th' 'that' 'the' 'there'\n",
            " 'this' 'thomas' 'times' 'to' 'types' 'uses' 'using' 'various' 'violets'\n",
            " 'voting' 'wall' 'was' 'we' 'will' 'with' 'wood' 'woodchuck' 'worst'\n",
            " 'would' 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "term_incidence_matrix = term_matrix.todense()\n",
        "print(term_incidence_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKtR4X1XooEy",
        "outputId": "ad1e5992-8338-4fc3-a935-597b687e9541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean_query(query, operation):\n",
        "    query_terms = preprocess(query)\n",
        "    # Initialize the query vector to match the term incidence matrix's dimensions\n",
        "    query_vector = np.zeros(term_incidence_matrix.shape[0], dtype=int)\n",
        "\n",
        "    for term in query_terms:\n",
        "        if term in terms:\n",
        "            term_index = terms.index(term)\n",
        "            query_vector[term_index] = 1  # Mark term as present\n",
        "\n",
        "\n",
        "    # Apply Boolean operations using bitwise operations on the matrix\n",
        "    if operation == \"AND\":\n",
        "        # For 'AND', all terms in the query must be present in a document\n",
        "        result_mask = np.prod(term_incidence_matrix[query_vector == 1, :], axis=0) == 1\n",
        "    elif operation == \"OR\":\n",
        "        # For 'OR', any of the terms can be present\n",
        "        result_mask = np.sum(term_incidence_matrix[query_vector == 1, :], axis=0) > 0\n",
        "    elif operation == \"NOT\":\n",
        "        # 'NOT' is more complex and often used with other conditions. This example assumes you want documents without the terms.\n",
        "        result_mask = np.sum(term_incidence_matrix[query_vector == 1, :], axis=0) == 0\n",
        "    else:\n",
        "        raise ValueError(\"Unknown operation\")\n",
        "\n",
        "    # Convert mask to boolean array to select matching documents\n",
        "    return result_mask.astype(bool)"
      ],
      "metadata": {
        "id": "Smo6_wKaldd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute an example query\n",
        "query = \"sells\"\n",
        "operation = \"AND\"\n",
        "result = boolean_query(query, operation)\n",
        "\n",
        "# Output matching documents\n",
        "print(\"Documents matching the query:\")\n",
        "for i, match in enumerate(result):\n",
        "    if match.all():\n",
        "        print(f\"Document {i} : {preprocessed_documents[i]} \" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vIEsRcknFBC",
        "outputId": "7e240128-1a00-4cf2-cb8e-344696b9ff66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents matching the query:\n",
            "Document 0 : she sells seashells by the seashore \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute an example query\n",
        "query = \"shore\"\n",
        "operation = \"NOT\"\n",
        "result = boolean_query(query, operation)\n",
        "\n",
        "# Output matching documents\n",
        "print(\"Documents matching the query:\")\n",
        "for i, match in enumerate(result):\n",
        "    if match.all():\n",
        "        print(f\"Document {i} : {preprocessed_documents[i]} \" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvradrInsAd_",
        "outputId": "1838129e-e5a6-426b-f79a-0bece06a3c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents matching the query:\n",
            "Document 0 : she sells seashells by the seashore \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusted preprocessing function to handle list input for documents and query\n",
        "def preprocess_documents(documents):\n",
        "    preprocessed_docs = []\n",
        "    for doc in documents:\n",
        "        # Remove punctuation and convert to lowercase\n",
        "        doc_no_punctuation = doc.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "        # Lemmatize the document\n",
        "        tokens = doc_no_punctuation.split()\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "        preprocessed_docs.append(' '.join(lemmatized_tokens))\n",
        "    return preprocessed_docs"
      ],
      "metadata": {
        "id": "7wyvczLWizum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the documents and the query\n",
        "preprocessed_documents = preprocess_documents(preprocessed_documents)\n",
        "#preprocessed_query = preprocess_documents(query)"
      ],
      "metadata": {
        "id": "AmL-UugMiR85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAK1lIC-hb_-",
        "outputId": "1375bb3a-2975-46c9-ade5-f95c55b9fd10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['she sell seashell by the seashore',\n",
              " 'how much wood would a woodchuck chuck if a woodchuck could chuck wood',\n",
              " 'to be or not to be that is the question',\n",
              " 'lorem ipsum dolor sit amet consectetur adipiscing elit',\n",
              " 'rose are red violet are blue sugar is sweet and so are you',\n",
              " 'the cat in the hat sat on the mat',\n",
              " 'all that glitter is not gold',\n",
              " 'it wa the best of time it wa the worst of time',\n",
              " 'in a hole in the ground there lived a hobbit',\n",
              " 'to be or not to be that is the question',\n",
              " 'the quick brown fox jump over the lazy dog',\n",
              " 'she sell seashell by the seashore',\n",
              " 'how much wood would a woodchuck',\n",
              " 'hey did you know that the summer break is coming amazing right it only more day',\n",
              " 'there are ball in this bag and in the other one',\n",
              " 'to be or not to be that is the question',\n",
              " 'the quick brown fox jump over the lazy dog',\n",
              " 'she sell seashell by the seashore',\n",
              " 'how much wood would a woodchuck chuck if a woodchuck could chuck wood',\n",
              " 'to be or not to be that is the question',\n",
              " 'we dont need the given question',\n",
              " 'this is a sample sentence and we are going to remove the stopwords from this',\n",
              " 'data science us scientific method algorithm and many type of process',\n",
              " 'n this article we will learn by using various python library and technique that are involved in text processing',\n",
              " 'there are bell in this bag and in the other one',\n",
              " 'gettysburg address by abraham lincoln',\n",
              " 'declaration of independence by thomas jefferson',\n",
              " 'emancipation proclamation by abraham lincoln',\n",
              " 'the magna carta',\n",
              " 'the bill of right the emancipation proclamation',\n",
              " 'the gettysburg address',\n",
              " 'emancipation proclamation',\n",
              " 'the th amendment',\n",
              " 'the social security act',\n",
              " 'the civil right act',\n",
              " 'the voting right act',\n",
              " 'the fair housing act',\n",
              " 'american with disability act',\n",
              " 'the affordable care act',\n",
              " 'doddfrank wall street reform and consumer protection act',\n",
              " 'the tax cut and job act']"
            ]
          },
          "metadata": {},
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d1f53c2"
      },
      "outputs": [],
      "source": [
        "# Compute the cosine similarity of the query to the documents\n",
        "cosine_sim = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b136aef",
        "outputId": "ac56cf68-ce10-4a76-a65b-c3a379621ebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.03362208 0.         0.02277647 0.         0.05456155 0.07122086\n",
            "  0.         0.02973195 0.02319943 0.02277647 0.04925204 0.03362208\n",
            "  0.         0.01646525 0.09282639 0.02277647 0.04925204 0.03362208\n",
            "  0.         0.02277647 0.02785309 0.07839427 0.06311954 0.0522114\n",
            "  0.09282639 0.         0.         0.         0.04193549 0.06002057\n",
            "  0.0462989  0.         0.04193549 0.16431563 0.17561996 0.17561996\n",
            "  0.16431563 0.10829698 0.16431563 0.15315506]]\n"
          ]
        }
      ],
      "source": [
        "print(cosine_sim )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fc1ce00",
        "outputId": "07f4d2d3-53ba-4ee4-da9b-d78269090ba3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Document 36': 0.17561996220870263,\n",
              " 'Document 35': 0.17561996220870263,\n",
              " 'Document 37': 0.1643156288199158,\n",
              " 'Document 34': 0.1643156288199158,\n",
              " 'Document 39': 0.1643156288199158,\n",
              " 'Document 40': 0.15315506445764465,\n",
              " 'Document 38': 0.10829698465106331,\n",
              " 'Document 25': 0.09282639294761394,\n",
              " 'Document 15': 0.09282639294761394,\n",
              " 'Document 22': 0.07839427424529716,\n",
              " 'Document 6': 0.0712208598155101,\n",
              " 'Document 23': 0.06311953663419147,\n",
              " 'Document 30': 0.06002057453625364,\n",
              " 'Document 5': 0.05456155355649612,\n",
              " 'Document 24': 0.05221140402628178,\n",
              " 'Document 17': 0.049252036974424825,\n",
              " 'Document 11': 0.049252036974424825,\n",
              " 'Document 31': 0.04629889705033121,\n",
              " 'Document 33': 0.041935492658273794,\n",
              " 'Document 29': 0.041935492658273794,\n",
              " 'Document 12': 0.03362208175014476,\n",
              " 'Document 18': 0.03362208175014476,\n",
              " 'Document 1': 0.03362208175014476,\n",
              " 'Document 8': 0.02973194771786725,\n",
              " 'Document 21': 0.02785308804944127,\n",
              " 'Document 9': 0.02319942872360029,\n",
              " 'Document 10': 0.02277646639333377,\n",
              " 'Document 3': 0.02277646639333377,\n",
              " 'Document 16': 0.02277646639333377,\n",
              " 'Document 20': 0.02277646639333377,\n",
              " 'Document 14': 0.016465245708567534,\n",
              " 'Document 27': 0.0,\n",
              " 'Document 26': 0.0,\n",
              " 'Document 13': 0.0,\n",
              " 'Document 7': 0.0,\n",
              " 'Document 28': 0.0,\n",
              " 'Document 32': 0.0,\n",
              " 'Document 4': 0.0,\n",
              " 'Document 2': 0.0,\n",
              " 'Document 19': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 258
        }
      ],
      "source": [
        "# Rank documents based on their cosine similarity to the query\n",
        "sorted_indices = np.argsort(cosine_sim[0])[::-1]\n",
        "ranked_documents = {f\"Document {index+1}\": cosine_sim[0][index] for index in sorted_indices}\n",
        "\n",
        "ranked_documents"
      ]
    }
  ]
}